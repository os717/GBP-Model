{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GBP.data import DataGenerator\n",
    "from GBP.gbp import run_GaBP_SYNC_ACCELERATED, run_GaBP_HARDWARE_BESTCASE_RESIDUAL, run_GaBP_HARDWARE_BESTCASE, run_GaBP_HARDWARE_ACCELERATED, run_GaBP_HARDWARE_ACCELERATED_RESIDUAL, run_GaBP_HARDWARE_ACCELERATED_EXCLUSION, run_GaBP_HARDWARE_ACCELERATED_EXCLUSION_NEIGHBOURS_ACROSS_STREAMS, run_GaBP_HARDWARE_ACCELERATED_EXCLUSION_NEIGHBOURS_ACROSS_STREAMS_STOCHASTIC\n",
    "# , run_GaBP_HARDWARE_ACCELERATED_EXCLUSION_NEIGHBOURS_ACROSS_2_STREAMS\n",
    "from GBP.utilities import HiddenPrints\n",
    "from GBP.visulisation import set_plot_options, get_plot_colors, NetworkxGraph, AnalyzeResult\n",
    " \n",
    "import warnings\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Option 1: Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_plot_options()\n",
    "colors = get_plot_colors()\n",
    "\n",
    "data_gen = DataGenerator()\n",
    "result_analyzer = AnalyzeResult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# def custom_sample(items, n):\n",
    "#     indices = random.sample(range(len(items)), n)\n",
    "#     return [items[i] for i in indices]\n",
    "\n",
    "# # Example usage\n",
    "# items = [\"apple\", \"banana\", \"orange\", \"grape\", \"kiwi\"]\n",
    "# n = 10\n",
    "# selected_items = custom_sample(items, n)\n",
    "# print(selected_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 200\n",
    "\n",
    "sync_convergence_threshold = 1*10**-5\n",
    "\n",
    "NODE_UPDT_PE =  20\n",
    "PEs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "low >= high",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m ASYNC_ITER \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(TRIES):\n\u001b[0;32m----> 8\u001b[0m     A, b \u001b[38;5;241m=\u001b[39m \u001b[43mdata_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_1D_line_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------- ITERATION: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# A, b = data_gen.get_2D_lattice_matrix(int(math.sqrt(num_nodes)), int(math.sqrt(num_nodes)))\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/oosha/Documents/PhD/GBP-Model/GBP_Simulations/GBP/data.py:219\u001b[0m, in \u001b[0;36mDataGenerator.get_1D_line_matrix\u001b[0;34m(self, dim, normalized, scaling)\u001b[0m\n\u001b[1;32m    216\u001b[0m A \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(A)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaling:\n\u001b[0;32m--> 219\u001b[0m     scaling_factors_A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Generate random scaling factors for each element in A\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     A \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m*\u001b[39m scaling_factors_A\n\u001b[1;32m    222\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(dim, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32mmtrand.pyx:765\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_bounded_integers.pyx:1247\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "sum_of_iterations = 0\n",
    "num_iterations = 1\n",
    "\n",
    "TRIES = 1000\n",
    "ASYNC_ITER = 10\n",
    "\n",
    "for it in range(TRIES):\n",
    "    A, b = data_gen.get_1D_line_matrix_PSD(num_nodes, scaling=True, normalized=False)\n",
    "    print(f\"---------- ITERATION: {it} ----------\")\n",
    "    # A, b = data_gen.get_2D_lattice_matrix(int(math.sqrt(num_nodes)), int(math.sqrt(num_nodes)))\n",
    "    A = data_gen.cut_random_edges(A, 0)\n",
    "    graph = NetworkxGraph(A)\n",
    "    P_i, mu_i, N_i, P_ii, mu_ii, P_ij, mu_ij, iter_dist, stand_divs, means, iteration = run_GaBP_SYNC_ACCELERATED(A, b, max_iter=100_000, mae=False, convergence_threshold=sync_convergence_threshold, show=True)\n",
    "    sum_of_iterations += iteration\n",
    "    final_mean = list(mu_i)\n",
    "    final_std = P_i\n",
    "    if iteration < 100000:\n",
    "        break\n",
    "    else:\n",
    "        iteration = float(\"inf\")\n",
    "        print(\"=========== RESTART ===========\")\n",
    "\n",
    "print(f\"NUMBER SYNC ITERATIONS = {iteration} using {it} tries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random - No Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED(A, b, caching=False, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    sum_of_iterations += iteration\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random - with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED(A, b, caching=True, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    sum_of_iterations += iteration\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random (Exclusion) - No Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "it = 0\n",
    "\n",
    "while (it < num_iterations):\n",
    "\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration, _ = run_GaBP_HARDWARE_ACCELERATED_EXCLUSION(A, b, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=100_000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    \n",
    "    if iteration > 0:\n",
    "        sum_of_iterations += iteration\n",
    "        print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "        it += 1\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random (Exclusion) - With Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration, _ = run_GaBP_HARDWARE_ACCELERATED_EXCLUSION(A, b, caching=True, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    sum_of_iterations += iteration\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual - with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED_RESIDUAL(A, b, caching=False, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    sum_of_iterations += iteration\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual - with Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED_RESIDUAL(A, b, caching=True, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    sum_of_iterations += iteration\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED_RESIDUAL(A, b, caching=True, capping=1, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    sum_of_iterations += iteration\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed - No Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    node_update_schedule = np.arange(num_nodes, dtype=np.int64)\n",
    "    np.random.shuffle(node_update_schedule)\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED(A, b, caching=False, mode='fixed', node_update_schedule_enter=node_update_schedule, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "    # print(f\"streams = {iteration}\")\n",
    "    # print(f\"node_update_schedule = {node_update_schedule}\")\n",
    "    sum_of_iterations += iteration\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed - Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_convergence_threshold = 1*10**-2\n",
    "\n",
    "sum_of_iterations = 0\n",
    "num_iterations = ASYNC_ITER\n",
    "\n",
    "for it in range(0,num_iterations):\n",
    "    node_update_schedule = np.arange(num_nodes, dtype=np.int64)\n",
    "    np.random.shuffle(node_update_schedule)\n",
    "    P_i, mu_i, iteration = run_GaBP_HARDWARE_ACCELERATED(A, b, caching=True, mode='fixed', node_update_schedule_enter=node_update_schedule, node_updates_per_pe=NODE_UPDT_PE, number_pes=PEs, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "    print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "    # print(f\"streams = {iteration}\")\n",
    "    # print(f\"node_update_schedule = {node_update_schedule}\")\n",
    "    sum_of_iterations += iteration\n",
    "\n",
    "print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO STREAMS (BESTCASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async_convergence_threshold = 1*10**-2\n",
    "\n",
    "# sum_of_iterations = 0\n",
    "# num_iterations = 100\n",
    "\n",
    "# for it in range(0,num_iterations):\n",
    "#     # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "#     P_i, mu_i, iteration = run_GaBP_HARDWARE_BESTCASE(A, b, node_updates_per_stream=NODE_UPDT_PE, TRUE_MEAN=final_mean, max_iter=10000, mae=False, convergence_threshold=async_convergence_threshold, show=True)\n",
    "#     sum_of_iterations += iteration\n",
    "#     print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "# print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO STREAMS (RESIDUAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async_convergence_threshold = 1*10**-2\n",
    "\n",
    "# sum_of_iterations = 0\n",
    "# num_iterations = 100\n",
    "\n",
    "# for it in range(0,num_iterations):\n",
    "#     # print(f\"-------------- ITERATION = {it+1} --------------\")\n",
    "#     P_i, mu_i, iteration = run_GaBP_HARDWARE_BESTCASE_RESIDUAL(A, b, node_updates_per_stream=NODE_UPDT_PE, TRUE_MEAN=final_mean, max_iter=500, mae=False, convergence_threshold=async_convergence_threshold, show=False)\n",
    "#     sum_of_iterations += iteration\n",
    "#     print(f\"-------------- ITERATION = {it+1} => Streams = {iteration} --------------\")\n",
    "\n",
    "# print(f\"AVE. ASYNC ITERATIONS = {sum_of_iterations/num_iterations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
